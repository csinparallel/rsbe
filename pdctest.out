Jobe server:  localhost:4001
Supported languages:
    c: 11.3.0
    cpp: 11.3.0
    java: 18.0.2
    nodejs: 12.22.9
    octave: 6.4.0
    pascal: 3.2.2
    pdc: 0.2
    php: 8.1.2
    python3: 3.10.6



Running C
Successful run

Output:
Hello world
Isn't this fun!



Running C++ (2)
Successful run

Output:
Hello Jobe!



Running PDC/gcc
Successful run

Output:
OMP defined, threadct = 8
With 1048576 trapezoids,  our estimate of the integral from 
0.000000 to 3.141593 is 2.000000



Running PDC/g++
Successful run

Output:
OMP defined, threadct = 8
With n = 1048576 trapezoids, our estimate of the integral from 0 to 3.14159 is 2



Running PDC/g++ (2)
Successful run

Output:
OMP defined, threadct = 8
With n = 1048576 trapezoids, our estimate of the integral from 0 to 3.14159 is 2



Running PDC/mpicc
Successful run

Output:
Greetings from process #0 of 4 on csinparallel
Greetings from process #1 of 4 on csinparallel
Greetings from process #2 of 4 on csinparallel
Greetings from process #3 of 4 on csinparallel



Running PDC/mpic++
Successful run

Output:
maximal score is 5, achieved by ligands 
hoach 



Running PDC/nvcc
Successful run

Output:
Device 0: "Quadro P2200" with Compute 6.1 capability
Total amount of global memory:      5053 MBytes (5298716672 bytes)
GPU device shared memory per block of threads on an SM: 49152 bytes
GPU device total number of streaming multiprocessors: 10
With  10 Multiprocessors (MPs), this device has 128 CUDA Cores/MP,
 for total of  1280 CUDA Cores on this device.

Max dimension sizes of a grid (x,y,z): (2147483647, 65535, 65535)
Max dimension sizes of a thread block (x,y,z): (1024, 1024, 64)



Running PDC/nvcc
Successful run

Output:
Grid Dimensions : {1, 1, 1} blocks. 
Block Dimensions : {8, 1, 1} threads.
From each thread:
I am thread (0, 0, 0) of block (0, 0, 0) in the grid
I am thread (1, 0, 0) of block (0, 0, 0) in the grid
I am thread (2, 0, 0) of block (0, 0, 0) in the grid
I am thread (3, 0, 0) of block (0, 0, 0) in the grid
I am thread (4, 0, 0) of block (0, 0, 0) in the grid
I am thread (5, 0, 0) of block (0, 0, 0) in the grid
I am thread (6, 0, 0) of block (0, 0, 0) in the grid
I am thread (7, 0, 0) of block (0, 0, 0) in the grid



Running PDC/pgcc
Successful run

Output:
./matrix_ex_float_acc total runtime 0.522188

===== STANDARD ERROR =====
pgcc-Warning-The flag -ta has been deprecated, please use -acc and -gpu instead.

"matrix_ex_float_acc.c", line 85: warning: variable "A" is used before its value is set [used_before_set]
     A = (float**)MakeMatrix(size, nr, nc, A);
                                           ^

Remark: individual warnings can be suppressed with "--diag_suppress <warning-name>"

"matrix_ex_float_acc.c", line 87: warning: variable "B" is used before its value is set [used_before_set]
     B = (float**)MakeMatrix(size, nr, nc, B);
                                           ^

"matrix_ex_float_acc.c", line 89: warning: variable "C" is used before its value is set [used_before_set]
     C = (float**)MakeMatrix(size, nr, nc, C);
                                           ^

MatrixMult:
     20, Generating copyin(A[:size-1][:size]) [if not already present]
         Generating copyout(C[:size][:size]) [if not already present]
         Generating copyin(B[:size][:size]) [if not already present]
     24, Loop is parallelizable
     26, Loop is parallelizable
         Generating NVIDIA GPU code
         24, #pragma acc loop gang, vector(128) collapse(2) /* blockIdx.x threadIdx.x */
         26,   /* blockIdx.x threadIdx.x auto-collapsed */
         29, #pragma acc loop seq
     29, Loop is parallelizable
copyMatrix:
     57, Generating copyout(A[:nr][:nc]) [if not already present]
         Generating copyin(B[:nr][:nc]) [if not already present]
     59, Loop is parallelizable
     61, Loop is parallelizable
         Generating NVIDIA GPU code
         59, #pragma acc loop gang, vector(128) collapse(2) /* blockIdx.x threadIdx.x */
         61,   /* blockIdx.x threadIdx.x auto-collapsed */
main:
     97, Generating copyin(A[:nr][:nc]) [if not already present]
         Generating copyout(C[:nr][:nc]) [if not already present]
         Generating copyin(B[:nr][:nc]) [if not already present]

